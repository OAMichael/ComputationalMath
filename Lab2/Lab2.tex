\include{./Include/Preambule.tex}
\newcommand{\tocsection}[1]{\section*{#1} \addcontentsline{toc}{section}{#1}}
\newcommand{\tocsubsection}[1]{\subsection*{#1} \addcontentsline{toc}{subsection}{#1}}
\renewcommand{\cftsecleader}{\cftdotfill{\cftdotsep}}
\setlength\extrarowheight{13pt}
\setcounter{secnumdepth}{0}

\begin{document}
		\include{./Include/Title.tex}
		\tableofcontents
		\newpage
		
		
		\tocsection{Цель}
		Реализовать численные методы решения систем линейных алгебраических уравнений (СЛАУ), сравнить полученные решения, оценить погрешности, найти число обусловленности в разных нормах, сделать печать невязок для методов решения, вычислить максимальное и минимальное собственные значения матрицы.
		
		
		\tocsection{Важные математические определения}
		Для начала приведем некоторые сведения про нормы, число обусловленности, и невязки.
		
		\tocsubsection{Нормы}
		Будем брать в расчет три разные нормы, определенные на пространстве, котором находятся $x$ и $f$, то есть нормы векторов:
		\begin{equation*}
			\|x\|_1 = \max\limits_k{|x_k|}
		\end{equation*}
		\begin{equation*}
			\|x\|_2 = \sum_{k}{|x_k|}
		\end{equation*}
		\begin{equation*}
			\|x\|_3 = \sqrt{(x, x)},
		\end{equation*}
		где операция $(\cdot\,, \cdot)$ -- скалярное произведение.
		
		Определим норму матрицы через норму вектора:
		\begin{equation*}
			\|A\| = \sup\limits_{x \neq 0}\frac{\|Ax\|}{\|x\|}
		\end{equation*}
		Отсюда ответвляются различные нормы матрицы, обоснованные на разных нормах вектора.
		
		\begin{equation*}
			\|A\|_1 = \max\limits_{1 \leqslant i \leqslant n}\sum_{j = 1}^{n}|a_{ij}|
		\end{equation*}
		\begin{equation*}
			\|A\|_2 = \max\limits_{1 \leqslant j \leqslant n}\sum_{i = 1}^{n}|a_{ij}|
		\end{equation*}
		\begin{equation*}
			\|A\|_3 = \sqrt{\max\limits_{1 \leqslant i \leqslant n}|\lambda^i(A^{^*} A)|}
		\end{equation*}
		
		
		Перейдем непосредственно к части вычислительной математики. 
		
		\tocsubsection{Число обусловленности}
		Определим число обусловленности для задачи $Ax = f$ -- число, показывающее, насколько хорошо обусловлена, поставлена задача:
		\begin{equation*}
			\mu = \|A\| \cdot \|A^{-1}\|
		\end{equation*}
		Причем неважно в какой норме. По известной теореме из курса математического анализа, все нормы в конечномерном пространстве эквивалентны.
		
		
		\tocsubsection{Невязки}
		Как в любом численном методе, полученное решение не является точным, поэтому необходимо оценивать ошибку по сравнению с действительным решением. Очевидный способ это сделать -- это невязка, определяемая следующим образом:
		\begin{equation*}
			r = \|Ax_\text{sol} - f\|,
		\end{equation*}
		где $x_\text{sol}$ -- решение, полученное численным методом. Как и с числом обусловленности, здесь неважно, какая норма будет использована.
		
		\tocsubsection{Критерий останова}
		Один из методов решения СЛАУ -- это итерационный метод, в котором $x^{k+1} = g(x^k)$, $x^k$ -- $k$-ая итерация. Однако хоть теоретически это можно делать бесконечно долго и бесконечно много раз, в реальности у компьютеров запас памяти, время на выполнение и другие ресурсы конечны. Именно поэтому необходимо сформулировать критерий, при котором следует остановить итерации. Самый банальный -- как только была достигнута необходимая точность. 
		
		Конкретно в данной работе определим критерий останова итераций как
		\begin{equation*}
			r = \|Ax_\text{sol} - f\| < \varepsilon,
		\end{equation*}
		где $\varepsilon$ -- нужная нам точность.
		
		\tocsubsection{Собственные значения}
		Определение собственных значений матрицы $A$ до банальности просто: это просто корни уравнения
		$\chi (\lambda) = \det(A - \lambda E) = 0$. 
		
		Нам, с точки зрения людей, занимающихся вычислительной математикой, важны собственные значения, являющиеся наибольшими и наименьшими по модулю.
		
		Для нахождения наибольшего по модулю собственного значения матрицы $A$ можно воспользоваться степенным методом.
		
		Пусть $y^{k+1} = Ay^k$. Тогда 
		\begin{equation*}
			|\lambda|_\text{max} = \lim\limits_{k \rightarrow \infty}\frac{\|y^{k+1}\|}{\|y^k\|}.
		\end{equation*}
	
		Хоть и выглядит просто, однако у этого алгоритма есть очень большой минус -- он не всегда работает, а именно зависим от начального приближения $y^0$. Самый простой пример $y^0$, при котором он не работает -- это если $y^0$ случайно выбран так, что он является собственным вектором, отвечающим другому собственному значению. В общем случае метод не будет работать для векторов $e_i$ таких, что $(e_i, e_{|\lambda|_\text{max}})_{A_\text{diag}} = 0$. Выглядит страшно, но это просто означает, что скалярное произведение с диагонализированной $A$ как матрицей Грама каких-то векторов $e_i$ с $e_{|\lambda|_\text{max}}$, отвечающих $|\lambda|_\text{max}$, равно нулю. Еще проще -- в базисе собственных векторов матрицы $A$ векторы $e_i$ не имеют компоненты вдоль $e_{|\lambda|_\text{max}}$, отвечающих $|\lambda|_\text{max}$. Дабы избежать этого, в скрипте выбирается начальный вектор $y^0 = (y_i^0),$ $y_i^0 = i,$ $i = \overline{1, n}$. 
		
		Другой минус -- это постоянное увеличение длины вектора на каждой итерации, вследствие чего может появиться переполнение. Решение этой проблемы -- это нормализация векторов. Однако это приводит к увеличению ошибки округления. 
		
		
		Существует такой интересный факт из линейной алгебры: если $\lambda$ -- собственное значение для матрицы $A$ такой, что $\det{A} \neq 0$, то $1/\lambda$ является собственным значением для матрицы $A^{-1}$.
		
		Основываясь на этом факте, мы можем построить алгоритм нахождения минимального по модулю собственного числа матрицы $A$. 
		
		Пусть $y^{k+1} = A^{-1}y^k$. Тогда
		\begin{equation*}
			1/|\lambda|_\text{min} = \lim\limits_{k \rightarrow \infty}\frac{\|y^{k+1}\|}{\|y^k\|}.
		\end{equation*} 
		
		
		Перейдем непосредственно к системе уравнений и методам ее решения.
		
			
		
		\newpage
		\tocsection{Система уравнений}
		В данной работе решается СЛАУ вида:
		\begin{equation*}
			Ax = f,
		\end{equation*}
		где $A = (a_{ij})$, $x = (x_i)$, $f = (f_i)$, где $i, j = \overline{1, n}$.
		
		Конкретно в этой работе система имеет вид (\textbf{пункт (к)}):
		\begin{equation*}
			n = 10; \; a_{ii} = 1; \; a_{ij} = \frac{1}{i + j}, \, i \neq j; \; f_i = \frac{1}{i}.
		\end{equation*}
		
		Предлагается решить систему двумя принципиально разными способами: прямой и итерационный.
		
		
		\tocsection{Прямой способ}
		В качестве прямого способа решения выбран метод Холецкого, поскольку $A = A^T$ -- симметричная положительно определенная матрица.
		
		Опишем алгоритм. Пусть $A = A^T$ -- симметричная положительно определенная матрица. Тогда $\exists L$ -- нижнетреугольная матрица такая, что $LL^T = A$.
		
		Так как $L$ -- нижнетреугольная, то получаются такие формулы для ее элементов:
		
		$l_{11} = \sqrt{a_{11}}$
		\vspace{2mm}
		
		$l_{j1} = \dfrac{a_{j1}}{l_{11}}, \;\; j \in \left[2, n\right]$
		\vspace{2mm}
		
		$l_{ii} = \sqrt{a_{ii} - \displaystyle\sum\limits_{p = 1}^{i-1} l_{ip}^2}, \;\; i \in \left[2, n\right]$
		\vspace{2mm}
		
		$l_{ji} = \dfrac{1}{l_{ii}}\left(a_{ji} - \displaystyle\sum\limits_{p=1}^{i-1} l_{ip} l_{jp} \right), \;\; i \in \left[2, n-1\right], \; j \in \left[i+1, n\right].$
		\vspace{2mm}
		
		Тогда решение исходной системы с симметричной матрицей разбивается на последовательное решение двух систем с треугольной матрицей:
		\begin{equation*}
			\boxed{LL^Tx = f} \xRightarrow[]{L^Tx\; =\; y} \boxed{Ly = f} \xRightarrow[]{\text{solved for }y} \boxed{L^Tx = y}
		\end{equation*}

		
	
		\newpage
		\tocsection{Итерационный метод}
		Выбранный итерационный метод -- метод верхней релаксации.
		
		Пусть матрица $A$ является положительно определенной, и $\omega \in (1, 2)$. Разобьем матрицу на три составляющие: $A = D + L + U$.
		\begin{itemize}
			\item[$\bullet$] $D = \text{diag}(A)$
			
			\item[$\bullet$] $L$ -- нижняя треугольная часть $A$, исключая диагональ
			
			\item[$\bullet$] $U$ -- верхняя треугольная часть $A$, исключая диагональ 
		\end{itemize}
		
		Тогда итерационный метод верхней релаксации выглядит следующим образом:
		\begin{equation*}
			x^{k+1} = -\left(D + \omega L\right)^{-1}\left[(\omega - 1)D + \omega U\right]x^k + \omega\left(D + \omega L\right)^{-1} \cdot f
		\end{equation*}
		
		
		Как уже говорилось выше, критерий останова для данного метода выберем $r = \|Ax_\text{sol} - f\| < \varepsilon$. Причем выберем норму $\| \cdot \| = \| \cdot \|_3$, а точность $\varepsilon = 10^{-16}$.
		
		Параметры: $\omega = 1.5, \;\; x^0 = (0 \;\; 0 \;\; \ldots \;\; 0)^T$.
		
		
		
		\newpage
		\tocsection{Результаты}
		Для начала посмотрим на результаты вычисления $\mu$ в разных нормах, максимальное и минимальное по модулю собственные значения матрицы $A$.
		
		$\mu_1 = \|A\|_1 \cdot \|A^{-1}\|_1 = 5.6336089382284005$ 
		
		$\mu_2 = \|A\|_2 \cdot \|A^{-1}\|_2 = 5.6336089382284005$ 
		
		$\mu_3 = \|A\|_3 \cdot \|A^{-1}\|_3 = 3.1131994246085670$ 
		
		\noindent Как видим, $\mu_i \ll 100$, следовательно, задача хорошо обусловлена.
		
		Теперь посмотрим на собственные значения. По расчету:
		
		$|\lambda|_\text{max} = 2.0483599269774470$
		
		$|\lambda|_\text{min} \, = 0.6579597538101798$
		
		\noindent Значения, полученные с помощью библиотеки NumPy:
		
		$|\lambda|_\text{max} = 2.0483599269774455$
		
		$|\lambda|_\text{min} \, = 0.6579597538101790$
		
		\noindent Видно, что результаты чрезвычайно близки друг к другу. Относительная ошибка $\thicksim 10^{-15}$.
		
		
		Теперь приступим к самому интересному -- сравним результаты, полученные разными методами, между собой.
		
			
		
	
		
		\newpage
		\thispagestyle{empty}
		\begin{landscape}
				\noindent\resizebox{1.6\textheight}{!}{
					$ x^\text{straight} = \begin{pmatrix}
					\;\;\; 0.9190771092669204 \\
					\;\;\; 0.1755401704930880 \\ 
					\;\;\; 0.0639348240144408 \\
					\;\;\; 0.0272747639608455 \\ 
					\;\;\; 0.0114234685355545 \\ 
					\;\;\; 0.0035108392787171 \\
					      -0.0007899578138555 \\
						  -0.0032508014494853 \\ 
						  -0.0046978778105116 \\ 
						  -0.0055537399412659 \\
					\end{pmatrix} \hspace{30mm} 
					x^\text{iterative} = \begin{pmatrix}
					\;\;\;  0.9190771092669205  \\
					\;\;\;  0.1755401704930880  \\
					\;\;\;  0.0639348240144407  \\
					\;\;\;  0.0272747639608455  \\
					\;\;\;  0.0114234685355545  \\
					\;\;\;  0.0035108392787171  \\
					 	   -0.0007899578138556  \\
						   -0.0032508014494853  \\
						   -0.0046978778105116  \\
						   -0.0055537399412659  \\
					\end{pmatrix}$
				}
			\vspace{10mm}
			
			\noindent\resizebox{1.1\linewidth}{!}{
				$
				\|x^\text{straight} - x^\text{iterative}\|_1 = 1.1102230246251565 \cdot 10^{-16} \hspace{10mm}
				\|x^\text{straight} - x^\text{iterative}\|_2 = 2.7961574028401160 \cdot 10^{-16} \hspace{10mm}
				\|x^\text{straight} - x^\text{iterative}\|_3 = 1.3389018904918906 \cdot 10^{-16} \hspace{10mm}
				$
			}
		
		\end{landscape}
		
		\newpage
	
		Результаты опять чрезвычайно близки. Различия появляются лишь в 16 знаке после запятой!
		
		Сделаем печать невязок обоих методов. Прямой метод:
		\begin{itemize}
			\item[$\bullet$] $\|Ax^\text{straight} - f\|_1 = 2.7755575615628914 \cdot 10^{-17}$ 
			
			\item[$\bullet$] $\|Ax^\text{straight} - f\|_2 = 1.1102230246251565 \cdot 10^{-16}$
			
			\item[$\bullet$] $\|Ax^\text{straight} - f\|_3 = 5.5511151231257830 \cdot 10^{-17}$
		\end{itemize}
	
		Итерационный метод:
		\begin{itemize}
			\item[$\bullet$] $\|Ax^\text{iterative} - f\|_1 = 2.7755575615628914 \cdot 10^{-17}$ 
			
			\item[$\bullet$] $\|Ax^\text{iterative} - f\|_2 = 1.1102230246251565 \cdot 10^{-16}$
			
			\item[$\bullet$] $\|Ax^\text{iterative} - f\|_3 = 5.5511151231257830 \cdot 10^{-17}$
		\end{itemize}
	
		По чистой случайности они совпали. Однако если выставить другую точность или другую норму для критерия останова, то значения у итерационного метода меняются.
		
		Для итерационного метода конкретно в этом случае потребовалось 77 итераций, чтобы достичь этого результата.
		
		
		\newpage
		\tocsection{Вывод}
		
		Были реализованы численные методы решения СЛАУ, а именно метод Холецкого в качестве прямого и метод верхней релаксации в качестве итерационного. Были найдены максимальное и минимальное по модулю собственные числа матрицы $A$:
		\begin{itemize}
			\item $|\lambda|_\text{max} = 2.0483599269774470$
			
			\item $|\lambda|_\text{min} \, = 0.6579597538101798$
		\end{itemize}
		
		Было вычислено число обусловленности в разных нормах:
		\begin{itemize}
			\item $\mu_1 = \|A\|_1 \cdot \|A^{-1}\|_1 = 5.6336089382284005$ 
		
			\item $\mu_2 = \|A\|_2 \cdot \|A^{-1}\|_2 = 5.6336089382284005$ 
		
			\item $\mu_3 = \|A\|_3 \cdot \|A^{-1}\|_3 = 3.1131994246085670$
	
		\end{itemize}
	
		Отсюда же видно, что задача хорошо обусловлена: $\mu_i \ll 100$.
		
		Решения, полученные разными способами, очень близки как друг к другу, так и к истинному, поскольку невязки составляют $\thicksim 10^{-16}$.


\end{document}